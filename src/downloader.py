import baostock as bs
import pandas as pd
import os
import datetime
import time
from multiprocessing import Pool, freeze_support
from tqdm import tqdm
from typing import List, Tuple, Optional, Any

# å¯¼å…¥é…ç½®
from config import (
    DATA_DIR, PROXY_URL, PROCESS_COUNT, MAX_ATTEMPTS,
    ABORT_THRESHOLD, DEFAULT_START_DATE, DATA_READY_HOUR
)


# ===========================
# è¾…åŠ©å‡½æ•° (ä¿æŒåœ¨ç±»å¤–éƒ¨ä»¥æ”¯æŒå¤šè¿›ç¨‹ Pickle)
# ===========================

def get_last_date(file_path: str) -> Optional[str]:
    """é«˜æ•ˆè¯»å– CSV æœ€åä¸€è¡Œæ—¥æœŸ"""
    try:
        if os.path.getsize(file_path) < 50:
            return None
        with open(file_path, 'rb') as f:
            try:
                f.seek(-2, os.SEEK_END)
                while f.read(1) != b'\n':
                    f.seek(-2, os.SEEK_CUR)
            except OSError:
                f.seek(0)
            last_line = f.readline().decode(errors='ignore')
            return last_line.split(',')[0]
    except Exception:
        return None


def set_proxy(enable: bool) -> None:
    if enable and PROXY_URL:
        os.environ["http_proxy"] = PROXY_URL
        os.environ["https_proxy"] = PROXY_URL
    else:
        os.environ.pop("http_proxy", None)
        os.environ.pop("https_proxy", None)


def check_status_worker(item: Tuple[str, str]) -> Tuple[Tuple[str, str], bool, Optional[str], Optional[str]]:
    """é¢„æ£€æŸ¥ Worker"""
    code, name = item
    safe_name = name.replace("*", "").replace("/", "").replace("?", "")
    file_path = os.path.join(str(DATA_DIR), f"{code}_{safe_name}.csv")

    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    current_hour = now.hour

    start_date = DEFAULT_START_DATE

    # 1. æ–‡ä»¶ä¸å­˜åœ¨ -> å…¨é‡ä¸‹è½½
    if not os.path.exists(file_path):
        return (item, True, start_date, 'w')

    last_date = get_last_date(file_path)
    if not last_date:
        return (item, True, start_date, 'w')

    # 2. åˆ¤å®šé€»è¾‘
    if last_date >= today_str:
        return (item, False, None, None)

    last_dt = datetime.datetime.strptime(last_date, "%Y-%m-%d")
    yesterday = (now - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

    # å¦‚æœæœ€åæ—¥æœŸæ˜¯æ˜¨å¤©ï¼Œä¸”ç°åœ¨è¿˜æ²¡åˆ°ä¸‹åˆ5ç‚¹ (æ”¶ç›˜æ•°æ®æœªå‡º) -> æ— éœ€æ›´æ–°
    if last_date == yesterday and current_hour < DATA_READY_HOUR:
        return (item, False, None, None)

    # è®¡ç®—å¢é‡æ›´æ–°çš„å¼€å§‹æ—¥æœŸ
    next_dt = last_dt + datetime.timedelta(days=1)
    new_start_date = next_dt.strftime("%Y-%m-%d")

    if new_start_date > today_str:
        return (item, False, None, None)

    # 3. éœ€è¦è¿½åŠ ä¸‹è½½ (æ¨¡å¼è®¾ä¸º 'a'ï¼Œä½†åœ¨ worker é‡Œæˆ‘ä»¬ä¼šåšå»é‡å¤„ç†)
    return (item, True, new_start_date, 'a')


def download_worker(args: Tuple[List[Any], bool]) -> Tuple[List[Any], List[Any]]:
    """ä¸‹è½½ Worker (åŒ…å«å»é‡é€»è¾‘)"""
    task_list, use_proxy = args
    set_proxy(use_proxy)

    fields = "date,code,open,high,low,close,volume,amount,adjustflag,turn,pctChg"
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")

    try:
        lg = bs.login()
        if lg.error_code != '0':
            return ([], [t[0] for t in task_list])
    except Exception:
        return ([], [t[0] for t in task_list])

    success = []
    failed = []

    for task in task_list:
        item, _, start_date, mode = task
        code, name = item
        safe_name = name.replace("*", "").replace("/", "").replace("?", "")
        file_path = os.path.join(str(DATA_DIR), f"{code}_{safe_name}.csv")

        try:
            rs = bs.query_history_k_data_plus(
                code, fields,
                start_date=start_date, end_date=today_str,
                frequency="d", adjustflag="2"
            )

            if rs.error_code != '0':
                failed.append(item)
                continue

            data_list = []
            while rs.next():
                data_list.append(rs.get_row_data())

            # åªæœ‰å½“è·å–åˆ°äº†æ•°æ®æ‰è¿›è¡Œå†™å…¥å¤„ç†
            if data_list:
                new_df = pd.DataFrame(data_list, columns=rs.fields)

                # æ•°å€¼è½¬æ¢
                cols_to_numeric = ['open', 'high', 'low', 'close', 'volume', 'pctChg']
                for col in cols_to_numeric:
                    if col in new_df.columns:
                        new_df[col] = pd.to_numeric(new_df[col], errors='coerce').fillna(0)

                # ã€å…³é”®ä¿®å¤ã€‘é˜²æ­¢é‡å¤è¿½åŠ å¯¼è‡´ MA çº¿ä¹±ç”»
                # å³ä½¿åŸæœ¬æ˜¯ 'a' æ¨¡å¼ï¼Œæˆ‘ä»¬ä¹Ÿè¯»å–æ—§æ–‡ä»¶ï¼Œåˆå¹¶ï¼Œå»é‡ï¼Œå†è¦†ç›–å†™å…¥
                if mode == 'a' and os.path.exists(file_path):
                    try:
                        old_df = pd.read_csv(file_path)
                        # åˆå¹¶
                        final_df = pd.concat([old_df, new_df])
                        # æ ¸å¿ƒï¼šæ ¹æ®æ—¥æœŸå»é‡ï¼Œä¿ç•™æœ€åä¸€æ¡
                        final_df.drop_duplicates(subset=['date'], keep='last', inplace=True)
                        final_df.sort_values('date', inplace=True)
                        final_df.to_csv(file_path, index=False)
                    except:
                        # å¦‚æœè¯»å–æ—§æ–‡ä»¶å¤±è´¥ï¼Œå°±ç›´æ¥è¦†ç›–
                        new_df.to_csv(file_path, index=False)
                else:
                    # 'w' æ¨¡å¼æˆ–æ–‡ä»¶ä¸å­˜åœ¨
                    new_df.to_csv(file_path, index=False)

            # åªè¦æ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼Œå°±ç®—æˆåŠŸï¼ˆå³ä½¿ data_list ä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰æ–°æ•°æ®ï¼Œä¹Ÿç®—ä»»åŠ¡å®Œæˆï¼‰
            success.append(item)
        except Exception:
            failed.append(item)

    bs.logout()
    return (success, failed)


# ===========================
# æ ¸å¿ƒç±»å®šä¹‰
# ===========================

class StockDownloader:
    def __init__(self):
        self.today_str = datetime.datetime.now().strftime("%Y-%m-%d")

    def get_all_stocks(self) -> List[Tuple[str, str]]:
        set_proxy(False)
        bs.login()
        print("ğŸ“‹ æ­£åœ¨è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
        stock_list = []
        for i in range(30):
            d = (datetime.datetime.now() - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
            rs = bs.query_all_stock(day=d)
            if rs.error_code != '0': continue
            temp = []
            while rs.next(): temp.append(rs.get_row_data())
            if len(temp) > 4000:
                for row in temp:
                    c, n = row[0], row[2]
                    if c.startswith(("sh.6", "sz.0", "sz.3", "bj.")):
                        stock_list.append((c, n))
                break
        bs.logout()
        return stock_list

    def _get_watchlist_stocks(self, watchlist_codes: List[str]) -> List[Tuple[str, str]]:
        tasks = []
        local_map = {}
        data_dir_str = str(DATA_DIR)
        if os.path.exists(data_dir_str):
            for f in os.listdir(data_dir_str):
                if f.endswith(".csv"):
                    try:
                        raw_name = f.replace(".csv", "")
                        parts = raw_name.split("_")
                        if len(parts) >= 2: local_map[parts[0]] = parts[1]
                    except:
                        continue
        for code in watchlist_codes:
            name = local_map.get(code, "è‡ªé€‰è‚¡")
            tasks.append((code, name))
        return tasks

    def run(self, target_codes: Optional[List[str]] = None):
        freeze_support()
        print(f"--- StockHunter æ•°æ®åŒæ­¥å¼•æ“ ---")

        tasks_to_run = []
        skipped_count = 0

        # 1. ç¡®å®šåˆ—è¡¨
        if target_codes:
            print(f"âš¡ æé€Ÿæ¨¡å¼ï¼šä»…æ›´æ–° {len(target_codes)} åªè‡ªé€‰è‚¡")
            all_stocks = self._get_watchlist_stocks(target_codes)
        else:
            all_stocks = self.get_all_stocks()

        if not all_stocks:
            print("âŒ æ²¡æœ‰è·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
            return

        # 2. é¢„æ£€
        print(f"\nğŸ” é¢„æ£€æœ¬åœ°æ–‡ä»¶çŠ¶æ€...")
        pool_size = 4 if target_codes else 8

        with Pool(processes=pool_size) as pool:
            results = pool.map(check_status_worker, all_stocks)

        for res in results:
            item, need_dl, start, mode = res
            if need_dl:
                tasks_to_run.append((item, need_dl, start, mode))
            else:
                skipped_count += 1

        print(f"â­ï¸  å·²è·³è¿‡: {skipped_count} | ğŸ“¥ å¾…ä¸‹è½½: {len(tasks_to_run)}")

        if not tasks_to_run:
            print("âœ… æ•°æ®å·²æ˜¯æœ€æ–°")
            return

        # 3. ä¸‹è½½å¾ªç¯
        pending_tasks = tasks_to_run
        final_success_count = 0

        # å»ºç«‹ä¸€ä¸ªé›†åˆæ¥è®°å½•å·²ç»å®Œæˆçš„ code (é¿å…ä¾èµ–ç£ç›˜æ ¡éªŒ)
        finished_codes = set()

        for attempt in range(1, MAX_ATTEMPTS + 1):
            # è¿‡æ»¤æ‰å·²ç»æˆåŠŸçš„ä»»åŠ¡
            current_batch = [t for t in pending_tasks if t[0][0] not in finished_codes]
            if not current_batch: break

            use_proxy = (attempt % 2 == 0)
            proxy_msg = f"ä»£ç†æ¨¡å¼" if use_proxy and PROXY_URL else "ç›´è¿æ¨¡å¼"
            print(f"\nğŸ”„ [ç¬¬ {attempt}/{MAX_ATTEMPTS} æ¬¡] {proxy_msg} | å‰©ä½™: {len(current_batch)}")

            chunk_size = 20
            chunks = []
            for i in range(0, len(current_batch), chunk_size):
                chunks.append((current_batch[i:i + chunk_size], use_proxy))

            consecutive_fail = 0
            abort = False

            with Pool(processes=PROCESS_COUNT) as pool:
                with tqdm(total=len(current_batch), desc="è¿›åº¦", unit="åª") as pbar:
                    for success_list, failed_list in pool.imap_unordered(download_worker, chunks):
                        pbar.update(len(success_list) + len(failed_list))

                        # ã€æ ¸å¿ƒä¿®å¤ã€‘ç›´æ¥ä½¿ç”¨ worker è¿”å›çš„æˆåŠŸåˆ—è¡¨
                        for item in success_list:
                            finished_codes.add(item[0])  # item[0] is code
                            final_success_count += 1

                        if success_list:
                            consecutive_fail = 0
                        else:
                            consecutive_fail += len(failed_list)

                        if consecutive_fail >= ABORT_THRESHOLD:
                            abort = True
                            pool.terminate()
                            break

            if abort:
                print(f"âš ï¸  è§¦å‘ç†”æ–­")
                break

            if len(finished_codes) < len(tasks_to_run) and attempt < MAX_ATTEMPTS:
                time.sleep(3)

        print(f"\nâœ… æ›´æ–°å®Œæˆ! æœ¬æ¬¡ä¸‹è½½: {final_success_count}")


if __name__ == "__main__":
    try:
        downloader = StockDownloader()
        downloader.run()
    except ImportError:
        pass
